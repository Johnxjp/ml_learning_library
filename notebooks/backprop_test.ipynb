{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw.network import MultiClassNN\n",
    "from raw.losses import cross_entropy_with_logits, d_cross_entropy_with_logits\n",
    "from raw.activations import softmax, relu, d_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_layers = [5]\n",
    "output_size = 2\n",
    "model = MultiClassNN(input_size, hidden_layers, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearANN {\n",
       "  Linear - weights: (10, 5), bias: (5,)\n",
       "  Linear - weights: (5, 2), bias: (2,)\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(positive_class_inds, batch_size, output_size):\n",
    "    y = np.zeros((batch_size, output_size), dtype=np.float)\n",
    "    y[np.arange(len(y)), positive_class_inds] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "x = np.random.normal(size=(batch_size, input_size))\n",
    "y = np.random.randint(0, 2, size=(batch_size,))\n",
    "print(y)\n",
    "y = one_hot(y, batch_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0.],\n",
       "        [0., 1.]]), array([[ 1.77979092e-01,  1.71495653e+00, -1.31959150e-03,\n",
       "         -6.34326476e-01,  7.81478045e-02, -1.20284598e+00,\n",
       "          7.45626552e-01, -1.41907464e-01, -1.68792807e+00,\n",
       "          5.60188521e-01],\n",
       "        [ 1.04216230e+00,  1.41963399e+00,  1.22515227e-01,\n",
       "          5.69501348e-01, -4.28790233e-01, -2.67170334e-02,\n",
       "         -1.17392385e+00,  1.03059318e-02,  4.73906365e-01,\n",
       "         -4.40513629e-01]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49383622, 0.50616378],\n",
       "       [0.63188709, 0.36811291]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_batch_per_neuron = cross_entropy_with_logits(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70555135, 0.        ],\n",
       "       [0.        , 0.99936558]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_batch_per_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss 0.8524584655973513\n"
     ]
    }
   ],
   "source": [
    "batch_loss = np.mean(np.sum(loss_per_batch_per_neuron, -1))\n",
    "print(\"Batch loss\", batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_error = d_cross_entropy_with_logits(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.02496285, -0.        ],\n",
       "       [-0.        , -2.71655784]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sense check. Remember if the loss error is negative then it means increasing the activation value will send the loss down and decreasing will increase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06281067, -0.03815431],\n",
       "        [ 0.48775348, -0.05256755]]), array([[1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.48775348, 0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99001028 0.00998972]\n",
      " [0.13584637 0.86415363]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99016409 0.00983591]\n",
      " [0.13437567 0.86562433]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9903138  0.0096862 ]\n",
      " [0.13293362 0.86706638]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99045957 0.00954043]\n",
      " [0.13151946 0.86848054]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99060154 0.00939846]\n",
      " [0.13013244 0.86986756]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99073986 0.00926014]\n",
      " [0.12877185 0.87122815]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99087465 0.00912535]\n",
      " [0.12743699 0.87256301]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99100604 0.00899396]\n",
      " [0.12612719 0.87387281]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99113415 0.00886585]\n",
      " [0.1248418  0.8751582 ]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9912591 0.0087409]\n",
      " [0.1235802 0.8764198]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99138099 0.00861901]\n",
      " [0.12234176 0.87765824]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99149994 0.00850006]\n",
      " [0.12112591 0.87887409]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99161604 0.00838396]\n",
      " [0.11993206 0.88006794]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99172938 0.00827062]\n",
      " [0.11875967 0.88124033]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99184007 0.00815993]\n",
      " [0.1176082  0.8823918 ]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99194818 0.00805182]\n",
      " [0.11647713 0.88352287]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9920538  0.0079462 ]\n",
      " [0.11536596 0.88463404]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99215702 0.00784298]\n",
      " [0.11427419 0.88572581]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9922579  0.0077421 ]\n",
      " [0.11320135 0.88679865]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99235653 0.00764347]\n",
      " [0.11214699 0.88785301]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99245297 0.00754703]\n",
      " [0.11111066 0.88888934]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9925473  0.0074527 ]\n",
      " [0.11009192 0.88990808]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99263957 0.00736043]\n",
      " [0.10909037 0.89090963]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99272986 0.00727014]\n",
      " [0.10810558 0.89189442]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99281821 0.00718179]\n",
      " [0.10713717 0.89286283]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99290469 0.00709531]\n",
      " [0.10618476 0.89381524]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99298936 0.00701064]\n",
      " [0.10524797 0.89475203]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99307227 0.00692773]\n",
      " [0.10432644 0.89567356]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99315347 0.00684653]\n",
      " [0.10341983 0.89658017]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99323301 0.00676699]\n",
      " [0.10252778 0.89747222]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99331093 0.00668907]\n",
      " [0.10164998 0.89835002]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99338729 0.00661271]\n",
      " [0.1007861  0.8992139 ]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99346213 0.00653787]\n",
      " [0.09993582 0.90006418]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99353549 0.00646451]\n",
      " [0.09909885 0.90090115]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99360741 0.00639259]\n",
      " [0.09827489 0.90172511]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99367793 0.00632207]\n",
      " [0.09746366 0.90253634]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99374709 0.00625291]\n",
      " [0.09666488 0.90333512]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99381493 0.00618507]\n",
      " [0.09587827 0.90412173]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99388148 0.00611852]\n",
      " [0.09510357 0.90489643]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99394678 0.00605322]\n",
      " [0.09434053 0.90565947]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99401086 0.00598914]\n",
      " [0.09358889 0.90641111]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99407376 0.00592624]\n",
      " [0.09284843 0.90715157]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99413549 0.00586451]\n",
      " [0.09211889 0.90788111]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9941961  0.0058039 ]\n",
      " [0.09140005 0.90859995]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99425561 0.00574439]\n",
      " [0.09069169 0.90930831]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99431405 0.00568595]\n",
      " [0.08999359 0.91000641]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99437145 0.00562855]\n",
      " [0.08930553 0.91069447]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99442784 0.00557216]\n",
      " [0.08862732 0.91137268]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99448323 0.00551677]\n",
      " [0.08795875 0.91204125]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99453766 0.00546234]\n",
      " [0.08729963 0.91270037]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99459114 0.00540886]\n",
      " [0.08664976 0.91335024]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99464371 0.00535629]\n",
      " [0.08600897 0.91399103]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99469538 0.00530462]\n",
      " [0.08537706 0.91462294]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99474617 0.00525383]\n",
      " [0.08475386 0.91524614]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99479611 0.00520389]\n",
      " [0.08413921 0.91586079]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99484522 0.00515478]\n",
      " [0.08353293 0.91646707]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99489351 0.00510649]\n",
      " [0.08293486 0.91706514]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.994941   0.005059  ]\n",
      " [0.08234484 0.91765516]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99498772 0.00501228]\n",
      " [0.08176271 0.91823729]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99503368 0.00496632]\n",
      " [0.08118834 0.91881166]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9950789  0.0049211 ]\n",
      " [0.08062156 0.91937844]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99512339 0.00487661]\n",
      " [0.08006223 0.91993777]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99516717 0.00483283]\n",
      " [0.07951022 0.92048978]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99521027 0.00478973]\n",
      " [0.07896538 0.92103462]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99525268 0.00474732]\n",
      " [0.07842758 0.92157242]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99529444 0.00470556]\n",
      " [0.0778967  0.9221033 ]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99533554 0.00466446]\n",
      " [0.0773726  0.9226274 ]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99537602 0.00462398]\n",
      " [0.07685515 0.92314485]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99541587 0.00458413]\n",
      " [0.07634425 0.92365575]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99545512 0.00454488]\n",
      " [0.07583976 0.92416024]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99549378 0.00450622]\n",
      " [0.07534158 0.92465842]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99553186 0.00446814]\n",
      " [0.07484958 0.92515042]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99556936 0.00443064]\n",
      " [0.07436367 0.92563633]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99560632 0.00439368]\n",
      " [0.07388373 0.92611627]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99564273 0.00435727]\n",
      " [0.07340965 0.92659035]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9956786  0.0043214 ]\n",
      " [0.07294133 0.92705867]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99571396 0.00428604]\n",
      " [0.07247867 0.92752133]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9957488  0.0042512 ]\n",
      " [0.07202158 0.92797842]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99578314 0.00421686]\n",
      " [0.07156995 0.92843005]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.995817   0.004183  ]\n",
      " [0.07112369 0.92887631]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99585037 0.00414963]\n",
      " [0.07068272 0.92931728]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99588327 0.00411673]\n",
      " [0.07024693 0.92975307]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99591571 0.00408429]\n",
      " [0.06981624 0.93018376]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9959477  0.0040523 ]\n",
      " [0.06939057 0.93060943]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99597924 0.00402076]\n",
      " [0.06896983 0.93103017]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99601035 0.00398965]\n",
      " [0.06855394 0.93144606]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99604103 0.00395897]\n",
      " [0.06814282 0.93185718]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.9960713  0.0039287 ]\n",
      " [0.06773638 0.93226362]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99610115 0.00389885]\n",
      " [0.06733456 0.93266544]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99613061 0.00386939]\n",
      " [0.06693727 0.93306273]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99615967 0.00384033]\n",
      " [0.06654444 0.93345556]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99618834 0.00381166]\n",
      " [0.066156   0.933844  ]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99621664 0.00378336]\n",
      " [0.06577188 0.93422812]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99624456 0.00375544]\n",
      " [0.06539201 0.93460799]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99627211 0.00372789]\n",
      " [0.06501632 0.93498368]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99629931 0.00370069]\n",
      " [0.06464475 0.93535525]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99632616 0.00367384]\n",
      " [0.06427722 0.93572278]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99635266 0.00364734]\n",
      " [0.06391368 0.93608632]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99637882 0.00362118]\n",
      " [0.06355406 0.93644594]] [[1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.99640464 0.00359536]\n",
      " [0.0631983  0.9368017 ]] [[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    out = model(x)\n",
    "    print(softmax(out), y)\n",
    "    neuron_loss = cross_entropy_with_logits(out, y)\n",
    "    batch_loss = np.mean(np.sum(neuron_loss, -1))\n",
    "    model.backward(y, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
